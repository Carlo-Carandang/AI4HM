{
 "metadata": {
  "name": "",
  "signature": "sha256:4f058fe82320906ebb6bcedc41fe0ef1e9fd966f5692426285879de7831fa704"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Exercise 2: Predictive accuracy\n",
      "\n",
      "(Reproducing ISL Figure 2.9 and 2.17)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
      "%matplotlib inline\n",
      "sns.set_context('poster')\n",
      "sns.set_style('darkgrid')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# set random seed, for reproducibility\n",
      "np.random.seed(12345)\n",
      "\n",
      "# funny little var for making notebook executable\n",
      "__________________ = None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# GPR example\n",
      "\n",
      "adapted from http://www.astroml.org/book_figures/chapter8/fig_gp_example.html"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# true x and y points\n",
      "x_true = np.linspace(0,10,500)\n",
      "y_true = np.cos(x_true)\n",
      "\n",
      "# noisy observed x and y points\n",
      "x_train = np.random.choice(x_true, 25, replace=False)\n",
      "dy = 0.5\n",
      "y_train = np.random.normal(np.cos(x_train), dy)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x_true, y_true, label='true')\n",
      "plt.plot(x_train, y_train, 's', label='observed')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from sklearn.gaussian_process import GaussianProcess"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "gp = GaussianProcess(theta0=100, nugget=.5)\n",
      "gp.fit(x_train[:, None], y_train)\n",
      "y_pred = gp.predict(x_true[:, None])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# What does x_train[:,None] do?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# What does the GPR prediction look like?\n",
      "plt.plot(x_true, y_true, label='true')\n",
      "plt.plot(x_train, y_train, 's', label='observed')\n",
      "plt.plot(x_true, y_pred, label='predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can measure the training error (RMSE):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = gp.predict(x_train[:, None])\n",
      "rmse = __________________\n",
      "print '(in-sample) rmse =', rmse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To measure the \"test\" error, we need different data.  Since this is a simulation, we can have as much as we want:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# noisy observed x and y points\n",
      "x_test= np.random.uniform(0, 10, size=1000)\n",
      "y_test = np.random.normal(np.cos(x_test), dy)\n",
      "\n",
      "# out-of-sample prediction\n",
      "y_pred = __________________\n",
      "rmse = __________________\n",
      "print '(out-of-sample) rmse =', rmse"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Refactor this into a function:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_train_error(theta0):\n",
      "    \"\"\" calculate the RMSE for the test and train data\n",
      "    using GPR with the specified theta0 value\"\"\"\n",
      "\n",
      "    gp = GaussianProcess(theta0=theta0, nugget=.5)\n",
      "    __________________\n",
      "    __________________\n",
      "    __________________\n",
      "    \n",
      "    return dict(__________________=__________________,)\n",
      "\n",
      "test_train_error(100)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "I didn't tell you about `dict` last time!  This is one of my favorite Python things.  It is like an array, but you can use whatever you want as the index.  Well, pretty much whatever you want...  in CS lingo, it is an *associative array*.  You can also create one with curly brackets:\n",
      "\n",
      "    d1 = {}\n",
      "    d1['abie'] = 'forgetful'\n",
      "    \n",
      "    d2 = {'yes':1, 'no':0}\n",
      "    d2['yes']  # what value will this produce?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Search over theta values to see how test and train error vary:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = []\n",
      "\n",
      "for theta0 in np.exp(np.linspace(-3, 5, 50)):\n",
      "    results.append(\n",
      "        test_train_error(theta0))\n",
      "    \n",
      "results = pd.DataFrame(results)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.semilogx(results.theta0, results.err_train, label='Train Error')\n",
      "plt.plot(results.theta0, results.err_test, label='Test Error')\n",
      "plt.legend()\n",
      "plt.xlabel('$\\theta_0$')\n",
      "plt.ylabel('RMSE')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So the best value is around $\\theta_0 = $ `__________________`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gp = GaussianProcess(theta0=__________________, nugget=.5)\n",
      "gp.fit(x_train[:, None], y_train)\n",
      "y_pred = gp.predict(x_true[:, None])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x_true, y_true, label='true')\n",
      "plt.plot(x_train, y_train, 's', label='observed')\n",
      "plt.plot(x_true, y_pred, label='predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# What does the prediction look like when $\\theta_0$ is too small?\n",
      "\n",
      "(Is it time to refactor this into a function?)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "gp = GaussianProcess(theta0=__________________, nugget=.5)\n",
      "__________________\n",
      "__________________\n",
      "__________________"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Same game for $k$-NN:\n",
      "\n",
      "But now the concept is *classification*, not *regression*."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv('ISL_Fig_2_9_data.csv', index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what is in df?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# fix the color names (data cleaning!)\n",
      "df['color'] = df['Y'].map({'red': 'orange', 'blue': 'purple'})"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# have a look at this data\n",
      "plt.figure(figsize=(8,8))\n",
      "for g, dfg in df.groupby('color'):\n",
      "    plt.plot(dfg['X.1'], dfg['X.2'], 's', color=g, ms=10, alpha=.6)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# load the k-NN library\n",
      "import sklearn.neighbors"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# create the training data\n",
      "\n",
      "# array of feature vectors\n",
      "X_train = np.array(df.filter(['X.1', 'X.2']))\n",
      "\n",
      "# corresponding labels\n",
      "y_train = np.array(df.color.map({'orange': 0, 'purple': 1}))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# fit a k-NN classifier\n",
      "n_neighbors=20\n",
      "weights='uniform'\n",
      "clf = sklearn.neighbors.KNeighborsClassifier(n_neighbors, weights)\n",
      "clf.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# explore the decision boundary. For that, we will predict each\n",
      "# point in the mesh [-3, 4]x[-3, 4].\n",
      "xx, yy = np.meshgrid(np.linspace(-3, 4, 500),\n",
      "                     np.linspace(-3, 4, 500))\n",
      "\n",
      "Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
      "Z = Z.reshape(xx.shape)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(8,8))\n",
      "for g, dfg in df.groupby('color'):\n",
      "    plt.plot(dfg['X.1'], dfg['X.2'], 's', color=g, ms=10, alpha=.6)\n",
      "plt.contour(xx, yy, Z, levels=[.5], colors='k', zorder=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# In-sample accuracy (concordance):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = clf.predict(X_train)\n",
      "\n",
      "c = __________________\n",
      "print '(in-sample) concordance =', c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To do out-of-sample, need to know distribution, or approximate it somehow. One way (to which we will return) is the train/test split.  In this simulation environment, however, we can just get more:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "orange_means = pd.read_csv('ISL_Fig_2_9_orange_mean.csv', index_col=0)\n",
      "purple_means = pd.read_csv('ISL_Fig_2_9_purple_mean.csv', index_col=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "To simulate data of either color, first choose a mean uniformly at random, and then draw from a normal centered at that mean, with standard deviation 0.5:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def rmixture(means):\n",
      "    i = np.random.randint(len(means.index))\n",
      "    return np.random.normal(loc=means.iloc[i], scale=.5)\n",
      "\n",
      "rmixture(orange_means), rmixture(purple_means)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "n_test = 1000\n",
      "\n",
      "X_test = [rmixture(orange_means) for i in range(n_test)] + \\\n",
      "           [rmixture(purple_means) for i in range(n_test)]\n",
      "X_test = np.array(X_test)\n",
      "\n",
      "y_test = [0 for i in range(n_test)] + [1 for i in range(n_test)]\n",
      "y_test = np.array(y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Have a look at this newly simulated data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.figure(figsize=(8,8))\n",
      "plt.plot(X_test[:n_test,0], X_test[:n_test,1], 's', color='orange', ms=10, alpha=.6)\n",
      "plt.plot(X_test[n_test:,0], X_test[n_test:,1], 's', color='purple', ms=10, alpha=.6)\n",
      "plt.contour(xx, yy, Z, levels=[.5], colors='k', zorder=10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "y_pred = __________________"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "c = __________________\n",
      "print '(out-of-sample) concordance =', c"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now refactor the test-/train-error measurement into a function that takes the $k$ in $k$-NN as a parameter, and sweep over a range of $k$ values to find the best $k$, and reproduce ISL Figure 2.17."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_train_error(k):\n",
      "    \"\"\" calculate the concordance for the test and train data\n",
      "    using k-NN with the specified k value\"\"\"\n",
      "    __________________\n",
      "    __________________\n",
      "    __________________\n",
      "    \n",
      "    return dict(__________________=__________________,)\n",
      "\n",
      "test_train_error(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "results = []\n",
      "\n",
      "__________________\n",
      "__________________\n",
      "__________________\n",
      "\n",
      "__________________"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# plot results, replication of figure 2.17 from ISL"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}