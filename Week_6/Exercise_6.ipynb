{
 "metadata": {
  "name": "",
  "signature": "sha256:e4f8953ca87f724907d85323e7f633766a5aaea27cb9ea973e3ad6cfa1675a33"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
      "%matplotlib inline\n",
      "sns.set_context('poster')\n",
      "sns.set_style('darkgrid')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set random seed, for reproducibility\n",
      "np.random.seed(12345)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Simulate data that is easy to plot\n",
      "\n",
      "This might not be the best example, but at least we can look at it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_true = np.linspace(0,15,1000)\n",
      "y_true = np.cos(x_true)\n",
      "\n",
      "sigma_true = .3\n",
      "x_train = np.random.choice(x_true, size=100)\n",
      "y_train = np.random.laplace(np.cos(x_train), sigma_true)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# remember what this looks like?\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# How do you think Linear Regression will perform in this setting?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.linear_model"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.linear_model.____________________"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_train = x_train[:,None]\n",
      "clf.fit(_____________________, _____________________)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(_____________________)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not so good, huh?\n",
      "\n",
      "# At this point in your life, do you have a number of tricks for improving this?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.preprocessing"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poly = sklearn.preprocessing.PolynomialFeatures(degree=_____________________)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poly.fit_transform(X_train)  # What does fit_transform seem to do?  is poly.fit different?  what about poly.transform?"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.linear_model.LinearRegression()\n",
      "clf.fit(poly.transform(X_train), y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(poly.transform(X_true))\n",
      "\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Adding squared terms doesn't do much, but how about higher degree polynomial?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "poly = sklearn.preprocessing.PolynomialFeatures(degree=_____________________)\n",
      "\n",
      "clf = sklearn.linear_model.LinearRegression()\n",
      "clf.fit(poly._____________________(X_train), y_train)\n",
      "\n",
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(poly._____________________(X_true))\n",
      "\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not so good if you have to extrapolate, though..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "x_true = np.linspace(-2,17,1000)\n",
      "y_true = np.cos(x_true)\n",
      "\n",
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(poly.transform(X_true))\n",
      "\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How about if you know that this is some sort of periodic function?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def my_transform(X, phases=[0,.5], freqs=[1., 2.]):\n",
      "    \"\"\" Return sine waves of a number of different phases and frequencies\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    X : _____________________\n",
      "    phases : _____________________\n",
      "    freqs : _____________________\n",
      "    \n",
      "    Results\n",
      "    -------\n",
      "    Returns _____________________\n",
      "    \"\"\"\n",
      "    \n",
      "    n,p = X.shape\n",
      "    assert p == 1, \"designed for univariate feature space only\"\n",
      "    X = X.reshape(n)\n",
      "    \n",
      "    X_new = np.empty((n,len(phases)*len(freqs)))\n",
      "    \n",
      "    col = 0\n",
      "    for a in phases:\n",
      "        for b in freqs:\n",
      "            X_new[:,col] = np.cos(a + b*X)\n",
      "            col += 1\n",
      "            \n",
      "    assert np.all(np.isfinite(X_new))\n",
      "    return X_new"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If you really know the phase and frequency, you can get it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phases = ________________________\n",
      "freqs = ________________________\n",
      "\n",
      "clf = sklearn.linear_model.LinearRegression()\n",
      "clf.fit(my_transform(X_train, phases, freqs), y_train)\n",
      "\n",
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(my_transform(X_true, phases, freqs))\n",
      "\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "But suppose that you do not have it so narrowed down:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phases=[0,1,2]\n",
      "freqs=np.linspace(0,6,7)\n",
      "\n",
      "clf = sklearn.linear_model.LinearRegression()\n",
      "clf.fit(my_transform(X_train, phases, freqs), y_train)\n",
      "\n",
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(my_transform(X_true, phases, freqs))\n",
      "\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And there is that whole small $n$ large $p$ thing you need to worry about:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phases=np.linspace(0,6,10)\n",
      "freqs=np.linspace(0,6,10)\n",
      "\n",
      "clf = sklearn.linear_model.LinearRegression()\n",
      "clf.fit(my_transform(X_train, phases, freqs), y_train)\n",
      "\n",
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(my_transform(X_true, phases, freqs))\n",
      "\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Which can end up totally breaking if $n$ is too small and $p$ is too large."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phases=np.linspace(0,6,25)\n",
      "freqs=np.linspace(0,6,25)\n",
      "\n",
      "clf = sklearn.linear_model.LinearRegression()\n",
      "clf.fit(my_transform(X_train, phases, freqs), y_train)\n",
      "\n",
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(my_transform(X_true, phases, freqs))\n",
      "\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Enter penalty functions, e.g. `sklearn.linear_model.Ridge`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phases=np.linspace(0,6,25)\n",
      "freqs=np.linspace(0,6,25)\n",
      "\n",
      "clf = sklearn.linear_model.Ridge(alpha=________________________)\n",
      "clf.fit(my_transform(X_train, phases, freqs), y_train)\n",
      "\n",
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(my_transform(X_true, phases, freqs))\n",
      "\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Or for this sort of application, `sklearn.linear_model.Lasso`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phases=np.linspace(0,6,25)\n",
      "freqs=np.linspace(0,6,25)\n",
      "\n",
      "clf = sklearn.linear_model.Lasso(alpha=________________________)\n",
      "clf.fit(my_transform(X_train, phases, freqs), y_train)\n",
      "\n",
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(my_transform(X_true, phases, freqs))\n",
      "\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Amazing, in my humble opinion!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# One thing that is so great about this?  You can understand the model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phases=np.linspace(0,6,100)\n",
      "freqs=np.linspace(0,6,100)\n",
      "\n",
      "clf = sklearn.linear_model.Lasso(alpha=.05)\n",
      "clf.fit(my_transform(X_train, phases, freqs), y_train)\n",
      "\n",
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(my_transform(X_true, phases, freqs))\n",
      "\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.______________________"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.coef_.________________________"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.where(clf.coef_>.01)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print phases[2:5]\n",
      "print freqs[15:18]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is the phase and frequency of the sine wave with a coefficient weight greater than 0.01?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "________________________, ________________________"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "One more penalty function, because I love the name: Elastic Nets"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phases=np.linspace(0,6,100)\n",
      "freqs=np.linspace(0,6,100)\n",
      "\n",
      "clf = sklearn.linear_model.ElasticNet(alpha=________________________, l1_ratio=________________________)\n",
      "clf.fit(my_transform(X_train, phases, freqs), y_train)\n",
      "\n",
      "X_true = x_true[:,None]\n",
      "y_pred = clf.predict(my_transform(X_true, phases, freqs))\n",
      "\n",
      "plt.plot(x_true, y_true, '-', label='Truth')\n",
      "plt.plot(x_train, y_train, 's', label='Train')\n",
      "plt.plot(x_true, y_pred, '-', label='Predicted')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.where(clf.coef_>.01)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How should you find the appropriate values of `alpha` (and `l1_ratio` in E-N case)?  Cross-validation, of course.  There are some special tricks for making this fast, and `sklearn` has implementations you can use if this is relevant to your projects."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Perhaps this all feels a little irrelevant\n",
      "\n",
      "And it is a bit of a stylized example.  In its defense, it looks pretty.  Also, I think I will be able to interest the astronomers in it.  But let us switch to a classification version of this:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# set random seed for reproducibility\n",
      "np.random.seed(12345)\n",
      "\n",
      "x_true = np.linspace(0,15,1000)\n",
      "y_true_numeric = np.cos(x_true)\n",
      "y_true = (np.random.uniform(low=-1, high=1, size=1000) <= y_true_numeric)\n",
      "\n",
      "train = np.random.choice(range(1000), size=100)\n",
      "x_train = x_true[train]\n",
      "y_train = y_true[train]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not as pretty..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x_true, y_true_numeric, '-', label='Propensity')\n",
      "plt.plot(x_train, np.random.normal(0,________________________,size=100)+y_train, 's', label='Jittered Train')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "That would look better if the class labels were $\\pm1$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_true = 2*y_true - 1\n",
      "y_train = y_true[train]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(x_true, y_true_numeric, '-', label='Propensity')\n",
      "plt.plot(x_train, np.random.normal(0,.01,size=100)+y_train, 's', label='Jittered Train')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# fit the linear model\n",
      "clf = sklearn.linear_model.LinearRegression()\n",
      "X_train = x_train[:,None]\n",
      "clf.fit(X_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make a categorical prediction with a linear regression\n",
      "y_pred = (clf.predict(X_train) >= ________________________)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# accuracy\n",
      "np.mean(________________________ == ________________________)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There must be something wrong with that, right???"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = ________________________"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean(y_pred == y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not very good, and that is in-sample fit!\n",
      "\n",
      "But we know how to fix it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "phases=np.linspace(0,6,100)\n",
      "freqs=np.linspace(0,6,100)\n",
      "\n",
      "clf = sklearn.linear_model.ElasticNet(alpha=.1, l1_ratio=________________________)\n",
      "clf.fit(my_transform(X_train, phases, freqs), y_train)\n",
      "\n",
      "y_pred = clf.predict(my_transform(X_train, phases, freqs))\n",
      "y_pred = (y_pred >= 0)\n",
      "y_pred = 2*y_pred -1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.mean(y_pred == y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "You should not be impressed by this until you test it out-of-sample... what do you think will happen in that case?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Using linear regression for this is a little silly, though.  We should consider a loss function that is more appropriate to the task at hand:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.linear_model.LogisticRegression(penalty=________________________, C=________________________)\n",
      "\n",
      "clf.fit(my_transform(X_train, phases, freqs), y_train)\n",
      "\n",
      "y_pred = clf.predict(my_transform(X_train, phases, freqs))\n",
      "# don't need to mess around with the results this time...\n",
      "\n",
      "np.mean(y_pred == y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Surprising that it is worse, not better.  Worth testing this out of sample, too, huh?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The hinge loss may be less familiar:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.svm"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.svm.SVC(kernel='linear')\n",
      "clf.fit(my_transform(X_train, phases, freqs), y_train)\n",
      "\n",
      "y_pred = clf.predict(my_transform(X_train, phases, freqs))\n",
      "# don't need to mess around with the results this time...\n",
      "\n",
      "np.mean(y_pred == y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The other cool thing about the SVM is the kernel trick.  It is possible to use the kernel trick in a lot of settings, but it is particularly central to SVM.  Let us first remove all of the transformed covariate columns we added:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.svm.SVC(kernel='linear', C=________________________)\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "y_pred = clf.predict(X_train)\n",
      "np.mean(y_pred == y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not good at all.  But with the kernel trick:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.svm.SVC(kernel='poly', C=_____________________, degree=_____________________)\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "y_pred = clf.predict(X_train)\n",
      "np.mean(y_pred == y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.svm.SVC(kernel='rbf', C=_____________________, gamma=_____________________)\n",
      "clf.fit(X_train, y_train)\n",
      "\n",
      "y_pred = clf.predict(X_train)\n",
      "np.mean(y_pred == y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Homework: use out-of-sample cross-validation to find the best values for C, gamma, and degree.  Use test/train/validation split to estimate the accuracy of the best method.  Bonus extra hard theory-and-application problem:  Figure out what the best accuracy possible is for this toy problem."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}