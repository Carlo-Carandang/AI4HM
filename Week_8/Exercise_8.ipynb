{
 "metadata": {
  "name": "",
  "signature": "sha256:80748ff49862f14479a83b0b73ff5763854321921bfdd4596274548a3257c3de"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns\n",
      "%matplotlib inline\n",
      "sns.set_context('paper')\n",
      "sns.set_style('darkgrid')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "PCA example as is often shown in books:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.random.multivariate_normal([0., 0.], [[1., .7],\n",
      "                                             [.7, 2]], size=1000)\n",
      "plt.plot(X[:,0], X[:,1], 'o', alpha=.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pretty straight-forward to do in `sklearn`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.decomposition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = sklearn.decomposition.PCA()\n",
      "Xt = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What does the following return?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "(pca.components_**2).sum(1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.subplot(2,2,1)\n",
      "plt.plot(X[:,0], X[:,1], 'o', alpha=.1)\n",
      "plt.arrow(0, 0, pca.components_[0,0], pca.components_[0,1], head_width=.2, linewidth=3, zorder=99)\n",
      "plt.arrow(0, 0, pca.components_[1,0], pca.components_[1,1], head_width=.2, linewidth=3, zorder=99)\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.plot(Xt[:,0], Xt[:,1], 'o', alpha=.1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sim_data(n=200, p=20, mu=None, sigma=None,\n",
      "             comp_pr=0, subs_pr=0, seed=123456):\n",
      "    \"\"\" data generation procedure\n",
      "    n : int, default=200\n",
      "      number of facilities\n",
      "    p : int, default=20\n",
      "      number of resource availability indicators\n",
      "    mu : list, default=[2,0,-2]\n",
      "      means for love mixture\n",
      "    sigma : list, default=[.6,.4,.6]\n",
      "      spreads for love mixture\n",
      "    comp_pr : float, default=0\n",
      "      fraction of resources to be complementary\n",
      "    subs_pr : float, default=0\n",
      "      fraction of resources to be substitutable\n",
      "    seed : int, default=123456\n",
      "      random seed, for reproducibility\n",
      "    \"\"\"\n",
      "    \n",
      "    # set random seed for reproducibility\n",
      "    np.random.seed(seed)\n",
      "\n",
      "    # means and spreads for the logit_love mixture distribution\n",
      "    if mu == None or sigma == None:\n",
      "        mu = [2, 0, -2]\n",
      "        sigma = [.6, .4, .6]\n",
      "\n",
      "    # sample logit_love from mixture of gaussians\n",
      "    cluster = np.empty(n, dtype=int)\n",
      "    logit_love = np.empty(n)\n",
      "\n",
      "    for i in range(n):\n",
      "        cluster[i] = np.random.choice(range(len(mu)))\n",
      "        logit_love[i] = np.random.normal(mu[cluster[i]], sigma[cluster[i]])\n",
      "    \n",
      "    # now generate preliminary resource availability indicators\n",
      "    logit_thing = np.random.normal(0., 2., size=p)\n",
      "\n",
      "    X = np.empty((n,p))\n",
      "    for i in range(n):\n",
      "        for j in range(p):\n",
      "            X[i,j] = np.random.uniform() < 1 / (1 + np.exp(-( \\\n",
      "                logit_love[i] + logit_thing[j])))\n",
      "\n",
      "\n",
      "    # now simulate complementarity and substitutability\n",
      "\n",
      "    complementary_to = np.empty(p)\n",
      "    for j in range(p):\n",
      "        if np.random.uniform() < comp_pr:\n",
      "            complementary_to[j] = np.random.choice(range(p))\n",
      "        else:\n",
      "            complementary_to[j] = np.nan\n",
      "\n",
      "    substitutable_to = np.empty(p)\n",
      "    for j in range(p):\n",
      "        if np.random.uniform() < subs_pr:\n",
      "            substitutable_to[j] = np.random.choice(range(p))\n",
      "        else:\n",
      "            substitutable_to[j] = np.nan\n",
      "\n",
      "    for i in range(n):\n",
      "        for j in range(p):\n",
      "            # only include a complementary resource if the resource it complements is present\n",
      "            j_prime = complementary_to[j]\n",
      "            if not np.isnan(j_prime):\n",
      "                if X[i,j_prime] == False:\n",
      "                    X[i,j] = False\n",
      "\n",
      "            # only include a substitutable resource if the resource that substitutes for it is absent\n",
      "            j_prime = substitutable_to[j]\n",
      "            if not np.isnan(j_prime):\n",
      "                if X[i,j_prime] == True:\n",
      "                    X[i,j] = False\n",
      "\n",
      "    results = pd.DataFrame(index=range(n))\n",
      "    results['cluster']=cluster\n",
      "    results['logit_love']=logit_love\n",
      "    for j in range(p):\n",
      "        results['X_%d'%j] = X[:,j]\n",
      "    \n",
      "    # stick all the other parameters into the results as class attributes, in case they are useful\n",
      "    results.mu = mu\n",
      "    results.sigma = sigma\n",
      "    results.logit_thing = logit_thing\n",
      "    results.substitutable_to = substitutable_to\n",
      "    results.complementary_to = complementary_to\n",
      "    return results\n",
      "sim_data(n=5, p=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What will the shape of the sim_data be for $n=19$ and $p=103$?  And will it take long to make?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sim_data(n=19, p=103).shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use a simulated dataset with a lot rows and a few columns for getting to know PCA:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = sim_data(n=1000, p=7)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.array(df.filter(like='X_'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(X.T + np.random.normal(0., .2, size=(7,1000)), alpha=.2)\n",
      ";"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(df.logit_love, X.sum(1), 'o')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So what will PCA do?  Let's see:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.decomposition"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = sklearn.decomposition.PCA()\n",
      "Xt = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How long does this take?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "pca = sklearn.decomposition.PCA()\n",
      "Xt = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What does it look like?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(Xt[:,0], Xt[:,1], 'o')\n",
      "plt.xlabel('$PC_0$')\n",
      "plt.ylabel('$PC_1$')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How much variance is explained by each principle component?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(pca.explained_variance_ratio_, 's-')\n",
      "plt.axis(xmin=-.5, xmax=6.5, ymax=.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And how does it compare the logit(love)?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(df.logit_love, Xt[:,0], 'o')\n",
      "plt.xlabel('logit(love)')\n",
      "plt.ylabel('$PC_0$')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The good news is that we have way more than 7 indicators of quality:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = sim_data(n=1000, p=70)\n",
      "X = np.array(df.filter(like='X_'))\n",
      "plt.plot(df.logit_love, X.sum(1), 'o')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How long do you think this will take now?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "pca = sklearn.decomposition.PCA()\n",
      "Xt = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What will plot of explained variance ratio look like?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(pca.explained_variance_ratio_, 's-')\n",
      "plt.axis(xmin=-.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And what do you think scatter of first two principle components will look like?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(Xt[:,0], Xt[:,1], 'o')\n",
      "plt.xlabel('$PC_0$')\n",
      "plt.ylabel('$PC_1$')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We should color that in...  We did start with a mixture model, after all, and that means we know the clusters."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for c, dfc in df.groupby('cluster'):\n",
      "    plt.plot(Xt[dfc.index, 0], Xt[dfc.index, 1], 'o', label='Cluster %s'%c)\n",
      "plt.xlabel('$PC_0$')\n",
      "plt.ylabel('$PC_1$')\n",
      "plt.legend(loc=(1,.1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And did I actually manage to improve the quality of my quality scores?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.subplot(2,2,1)\n",
      "plt.plot(df.logit_love, X.sum(1), 'o', alpha=.1)\n",
      "plt.xlabel('logit(love)')\n",
      "plt.ylabel('number of resources')\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.plot(df.logit_love, Xt[:,0], 'o', alpha=.1)\n",
      "plt.xlabel('logit(love)')\n",
      "plt.ylabel('$PC_0$')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It does not seem so, at least, not dramatically."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Let's use this setup to investigate the importance of scaling:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# change the units of df.X_0, so that it is 100X larger\n",
      "df.X_0 *= 100."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "np.all(df['X_0'] == df.X_0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.X_71 = 'abie'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.X_71"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.mu"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What will happen now?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X = np.array(df.filter(like='X_'))\n",
      "pca = sklearn.decomposition.PCA()\n",
      "Xt = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What will the explained variance ratio plot look like?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.plot(pca.explained_variance_ratio_, 's-')\n",
      "plt.axis(xmin=-.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What will the cluster-colored scatter of the first two principle components look like?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for c, dfc in df.groupby('cluster'):\n",
      "    plt.plot(Xt[dfc.index, 0], Xt[dfc.index, 1], 'o', label='Cluster %s'%c, alpha=.1)\n",
      "plt.xlabel('$PC_0$')\n",
      "plt.ylabel('$PC_1$')\n",
      "plt.legend(loc=(1,.1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What is the moral of that story?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Scaling matters a lot!"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now, can I take another opportunity to demonstrate the kernel stuff?  Not really...  this example does not make a good case for the utility of kernel PCA."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = sim_data(n=1000, p=70, mu=[0], sigma=[2], subs_pr=.6, comp_pr=.5)\n",
      "X = np.array(df.filter(like='X_'))\n",
      "plt.plot(df.logit_love, X.sum(1), 'o')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Have a look at plain, old PCA on this data:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = sklearn.decomposition.PCA()\n",
      "Xt = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.subplot(2,2,1)\n",
      "plt.plot(df.logit_love, X.sum(1), 'o', alpha=.1)\n",
      "plt.xlabel('logit(love)')\n",
      "plt.ylabel('number of resources')\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.plot(df.logit_love, Xt[:,0], 'o', alpha=.1)\n",
      "plt.xlabel('logit(love)')\n",
      "plt.ylabel('$PC_0$')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Compare that the kernel PCA:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "pca = sklearn.decomposition.KernelPCA(n_components=2, kernel='rbf', gamma=.001)\n",
      "Xt = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.subplot(2,2,1)\n",
      "plt.plot(df.logit_love, X.sum(1), 'o', alpha=.1)\n",
      "plt.xlabel('logit(love)')\n",
      "plt.ylabel('number of resources')\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "plt.plot(df.logit_love, Xt[:,0], 'o', alpha=.1)\n",
      "plt.xlabel('logit(love)')\n",
      "plt.ylabel('$PC_0$')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And here is a custom kernel, that I was messing around with:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.metrics.pairwise"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "pca = sklearn.decomposition.KernelPCA(n_components=2, kernel='precomputed')\n",
      "D = sklearn.metrics.pairwise.pairwise_distances(X, metric='minkowski', p=.9)\n",
      "Xt = pca.fit_transform(D)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Factor Analysis\n",
      "\n",
      "A little bit different from PCA, but think of them as very similar: http://stats.stackexchange.com/questions/123063/is-there-any-good-reason-to-use-pca-instead-of-efa"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = sim_data(n=1000, p=70, mu=[-1,2], sigma=[.5,.5])\n",
      "X = np.array(df.filter(like='X_'))\n",
      "plt.plot(df.logit_love, X.sum(1), 'o')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = sklearn.decomposition.PCA()\n",
      "Xt_pca = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "efa = sklearn.decomposition.FactorAnalysis()\n",
      "Xt = efa.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Have a look at how they compare:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.subplot(2,2,1)\n",
      "for c, dfc in df.groupby('cluster'):\n",
      "    plt.plot(Xt_pca[dfc.index, 0], Xt_pca[dfc.index, 1], 'o', label='Cluster %s'%c, alpha=.6)\n",
      "plt.xlabel('$PC_0$')\n",
      "plt.ylabel('$PC_1$')\n",
      "\n",
      "plt.subplot(2,2,2)\n",
      "for c, dfc in df.groupby('cluster'):\n",
      "    plt.plot(Xt[dfc.index, 0], Xt[dfc.index, 1], 'o', label='Cluster %s'%c, alpha=.6)\n",
      "plt.xlabel('$F_0$')\n",
      "plt.ylabel('$F_1$')\n",
      "plt.legend(loc=(1,.1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There are a ton of interest ways to embed high-dimensional data in low-dimensional spaces.  I will do more next week if it seems like your projects will benefit from them..."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.manifold"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Read more: http://en.wikipedia.org/wiki/Multidimensional_scaling ; http://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-components-analysis-and-multidimensional"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "mds = sklearn.manifold.MDS()\n",
      "Xt = mds.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for c, dfc in df.groupby('cluster'):\n",
      "    plt.plot(Xt[dfc.index, 0], Xt[dfc.index, 1], 'o', label='Cluster %s'%c)\n",
      "plt.xlabel('$Y_0$')\n",
      "plt.ylabel('$Y_1$')\n",
      "plt.legend(loc=(1,.1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Not to mention ways to embed low-dimensional data into high-dimensional spaces: "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.ensemble"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Read more, if interested: http://scikit-learn.org/stable/modules/ensemble.html#totally-random-trees-embedding"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rte = sklearn.ensemble.RandomTreesEmbedding()\n",
      "Xt = rte.fit_transform(X)\n",
      "\n",
      "Xt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Let's shift now to clustering"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.cluster"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K = 2\n",
      "clf = sklearn.cluster.KMeans(n_clusters=K, n_init=100, n_jobs=2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%time\n",
      "\n",
      "clf.fit(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "y_pred = clf.predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use PCA to look at it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pca = sklearn.decomposition.PCA()\n",
      "Xt = pca.fit_transform(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in range(K):\n",
      "    i = np.where(y_pred == k)[0]\n",
      "    plt.plot(Xt[i,0], Xt[i,1], 'o', label='Cluster %d'%k, alpha=.5)\n",
      "plt.legend(loc=(1,.1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can hack max_iter to make a flipbook of how this works:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K =5\n",
      "clf = sklearn.cluster.KMeans(n_clusters=K, max_iter=1)\n",
      "y_pred = clf.fit_predict(X)\n",
      "\n",
      "for k in range(K):\n",
      "    i = np.where(y_pred == k)[0]\n",
      "    plt.plot(Xt[i,0], Xt[i,1], 'o', label='Cluster %d'%k, alpha=.5)\n",
      "    \n",
      "plt.legend(loc=(1,.1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Clear-ish? Big question: how to pick $K$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "# try a range\n",
      "y_pred = {}\n",
      "\n",
      "for K in range(1, 10):\n",
      "    clf = sklearn.cluster.KMeans(n_clusters=K)\n",
      "    y_pred[K] = clf.fit_predict(X)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# make a clustergram\n",
      "cluster_means = {}\n",
      "for k in y_pred.keys():\n",
      "    g = df.filter(like='X_').groupby(y_pred[k])\n",
      "    cluster_means[k] = g.mean().mean(axis=1)\n",
      "    \n",
      "k = pd.Series(range(1,10))\n",
      "for i in range(len(y_pred[1])):\n",
      "    yy = [cluster_means[kk][y_pred[kk][i]]*np.exp(np.random.normal(0, .02)) for kk in k]  # why is that so complicated???\n",
      "    plt.plot(k, yy, 'k-', alpha=.1)\n",
      "\n",
      "plt.xlabel('Number of clusters ($K$)')\n",
      "plt.ylabel('Cluster Mean of Means')\n",
      "plt.title('Clustergram!')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "TODO: Refactor this into a function that takes a list of clusterings."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Let's try agglomerative clustering"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.cluster.AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
      "y_pred = clf.fit_predict(X)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for k in np.unique(y_pred):\n",
      "    i = np.where(y_pred == k)[0]\n",
      "    plt.plot(Xt[i,0], Xt[i,1], 'o', label='Cluster %d'%k)\n",
      "    \n",
      "plt.legend(loc=(1,.1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good time to refactor, and to experiment with a few other values for affinity and for linkage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def plot_clusters():\n",
      "    for k in np.unique(y_pred):\n",
      "        i = np.where(y_pred == k)[0]\n",
      "        plt.plot(Xt[i,0], Xt[i,1], 'o', label='Cluster %s'%str(k))\n",
      "\n",
      "    plt.legend(loc=(1,.1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.cluster.AgglomerativeClustering(n_clusters=3, affinity='l1', linkage='complete')\n",
      "y_pred = clf.fit_predict(X)\n",
      "\n",
      "plot_clusters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.cluster.AgglomerativeClustering(n_clusters=3, affinity='cosine', linkage='average')\n",
      "y_pred = clf.fit_predict(X)\n",
      "\n",
      "plot_clusters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pretty clear that distance matters (linkage matters, too...)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.metrics.pairwise"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = sklearn.metrics.pairwise.pairwise_distances(X, metric='jaccard')\n",
      "X.shape, D.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.cluster.AgglomerativeClustering(n_clusters=3, affinity='precomputed', linkage='complete')\n",
      "y_pred = clf.fit_predict(D)\n",
      "\n",
      "plot_clusters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import scipy.spatial"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "help(scipy.spatial.distance.minkowski)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = sklearn.metrics.pairwise.pairwise_distances(X, metric='minkowski', p=.5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.cluster.AgglomerativeClustering(n_clusters=3, affinity='precomputed', linkage='average')\n",
      "y_pred = clf.fit_predict(D)\n",
      "\n",
      "plot_clusters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A few other ways:\n",
      "\n",
      "Affinity Propagation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.cluster.AffinityPropagation(damping=.99)\n",
      "y_pred = clf.fit_predict(X)\n",
      "plot_clusters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.cluster.AffinityPropagation(damping=.95, affinity='precomputed')\n",
      "y_pred = clf.fit_predict(D)\n",
      "plot_clusters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "DBSCAN"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.cluster.DBSCAN(eps=10)\n",
      "y_pred = clf.fit_predict(X)\n",
      "plot_clusters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf = sklearn.cluster.DBSCAN(eps=100, metric='precomputed')\n",
      "y_pred = clf.fit_predict(D)\n",
      "plot_clusters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Finally, for the statisticians, an actual statistical model (Gaussian Mixture Model):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.mixture"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K=2\n",
      "clf = sklearn.mixture.GMM(n_components=K)\n",
      "clf.fit(X)\n",
      "y_pred = clf.predict(X)\n",
      "plot_clusters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K=2\n",
      "clf = sklearn.mixture.DPGMM(n_components=K)\n",
      "clf.fit(X)\n",
      "y_pred = clf.predict(X)\n",
      "plot_clusters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "K=2\n",
      "clf = sklearn.mixture.VBGMM(n_components=K)\n",
      "clf.fit(X)\n",
      "y_pred = clf.predict(X)\n",
      "plot_clusters()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here is the jackpot of distances available in sklearn:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sklearn.metrics.pairwise\n",
      "\n",
      "help(sklearn.metrics.pairwise.pairwise_distances)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}